{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colornet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UneR_LVS5wSg",
        "colab_type": "code",
        "outputId": "e6b9350e-c2a1-45cb-917e-770a3d3f9215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "directory = '/content/gdrive/My Drive/ColorNetData/'\n",
        "train_directory = directory + 'Train/'\n",
        "test_directory = directory + 'Test/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTl_c73aD7SC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import statements\n",
        "from os import listdir\n",
        "from pickle import load, dump\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, UpSampling2D, Input, RepeatVector, Reshape, concatenate\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
        "from keras.utils import plot_model\n",
        "from PIL import Image\n",
        "\n",
        "inception = InceptionResNetV2(include_top=True, weights='imagenet')\n",
        "dump(inception, open(directory + 'inception.p', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq8dQE1UER6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Skip if done\n",
        "# Create image embeddings\n",
        "inception = InceptionResNetV2(include_top=True, weights='imagenet')\n",
        "\n",
        "train_image_encoding = dict()\n",
        "for name in tqdm(listdir(train_directory)):\n",
        "  if not name.endswith('.jpg'):\n",
        "    continue\n",
        "  filename = train_directory + name\n",
        "  image = load_img(filename, target_size=(299, 299))\n",
        "  image = np.array(img_to_array(image), dtype=float)\n",
        "  image = gray2rgb(rgb2gray(image))\n",
        "  image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
        "  image = preprocess_input(image)\n",
        "  feature = inception.predict(image)\n",
        "  train_image_encoding[name] = feature\n",
        "  \n",
        "dump(train_image_encoding, open(directory + 'train_image_encoding.p', 'wb'))\n",
        "\n",
        "test_image_encoding = dict()\n",
        "for name in tqdm(listdir(test_directory)):\n",
        "  if not name.endswith('.jpg'):\n",
        "    continue\n",
        "  filename = test_directory + name\n",
        "  image = load_img(filename, target_size=(299, 299))\n",
        "  image = np.array(img_to_array(image), dtype=float)\n",
        "  image = gray2rgb(rgb2gray(image))\n",
        "  image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
        "  image = preprocess_input(image)\n",
        "  feature = inception.predict(image)\n",
        "  test_image_encoding[name] = feature\n",
        "\n",
        "dump(test_image_encoding, open(directory + 'test_image_encoding.p', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzKLgGr0JNda",
        "colab_type": "code",
        "outputId": "c89936fe-a42c-48d7-e597-450a77997c0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Load the training and testing data\n",
        "train_image_encoding = load(open(directory + 'train_image_encoding.p', 'rb'))\n",
        "test_image_encoding = load(open(directory + 'test_image_encoding.p', 'rb'))\n",
        "\n",
        "print(\"Length of training data : \", len(train_image_encoding))\n",
        "print(\"Length of testing data : \", len(test_image_encoding))\n",
        "\n",
        "print(\"Encoding shape : \", list(train_image_encoding.values())[0].shape) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training data :  8396\n",
            "Length of testing data :  396\n",
            "Encoding shape :  (1, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo5-eu11nPpJ",
        "colab_type": "code",
        "outputId": "d0fec996-a1c6-49a3-a189-11e0f99ae00a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Skip if once done\n",
        "# Model\n",
        "# Image encoding model\n",
        "image_encoder_input = Input(shape=(256, 256, 1))\n",
        "image_encoder = Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)(image_encoder_input)\n",
        "image_encoder = Conv2D(128, (3,3), activation='relu', padding='same')(image_encoder)\n",
        "image_encoder = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(image_encoder)\n",
        "image_encoder = Conv2D(256, (3,3), activation='relu', padding='same')(image_encoder)\n",
        "image_encoder = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(image_encoder)\n",
        "image_encoder = Conv2D(512, (3,3), activation='relu', padding='same')(image_encoder)\n",
        "image_encoder = Conv2D(512, (3,3), activation='relu', padding='same')(image_encoder)\n",
        "image_encoder = Conv2D(256, (3,3), activation='relu', padding='same')(image_encoder)\n",
        "\n",
        "# Image embedding model for inception\n",
        "image_embedder = Input(shape=(1000,))\n",
        "\n",
        "# Final encoder model using fusion\n",
        "fusion_output = RepeatVector(32 * 32)(image_embedder)\n",
        "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
        "fusion_output = concatenate([image_encoder, fusion_output], axis=3) \n",
        "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\n",
        "\n",
        "# Decoder model\n",
        "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "\n",
        "# Building the final model\n",
        "model = Model(inputs=[image_encoder_input, image_embedder], outputs=decoder_output)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
        "model.save_weights(directory + 'weights.h5')\n",
        "dump(model, open(directory + 'final_model.p', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q2fXCaQkBpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model and visualize\n",
        "model = load(open(directory + 'final_model.p', 'rb'))\n",
        "plot_model(model, to_file=directory + 'final_model.png', show_shapes=True)\n",
        "Image.open(directory + 'final_model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpdky8yek5ZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(batch_size=32):\n",
        "  encoder_input = list()\n",
        "  embedding_input = list()\n",
        "  model_output = list()\n",
        "  from itertools import cycle\n",
        "  for name, embedding in cycle(train_image_encoding.items()):\n",
        "    # Encoded image input\n",
        "    image = np.array(img_to_array(load_img(train_directory + name, target_size=(256, 256))), dtype=float)\n",
        "    encoded_image = rgb2lab(1.0/255*image)[:, :, 0]\n",
        "    encoded_image = encoded_image.reshape(encoded_image.shape[0], encoded_image.shape[1], 1)\n",
        "    # Output image\n",
        "    output_image = rgb2lab(1.0/255*image)[:, :, 1:]\n",
        "    output_image = output_image/128\n",
        "    output_image = output_image.reshape(output_image.shape[0], output_image.shape[1], 2)\n",
        "    # Add to return\n",
        "    encoder_input.append(encoded_image)\n",
        "    embedding_input.append(embedding.flatten())\n",
        "    model_output.append(output_image)\n",
        "    \n",
        "    if len(model_output) >= batch_size:\n",
        "      encoder_input = np.asarray(encoder_input)\n",
        "      embedding_input = np.asarray(embedding_input)\n",
        "      model_output = np.asarray(model_output)\n",
        "      yield [[encoder_input, embedding_input], model_output]\n",
        "      encoder_input = list()\n",
        "      embedding_input = list()\n",
        "      model_output = list()\n",
        "# Save the generator\n",
        "dump(data_generator, open(directory + 'data_generator.p', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suuuAqMrowXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile model and save weights\n",
        "batch_size = 32\n",
        "total_epochs = 1000\n",
        "steps_per_epoch = int(len(train_image_encoding)/batch_size)\n",
        "model.fit_generator(data_generator(batch_size), nb_epoch=1, verbose=1, steps_per_epoch=steps_per_epoch)\n",
        "model.save_weights(directory + 'weights.h5')\n",
        "print('Weights saved!')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}